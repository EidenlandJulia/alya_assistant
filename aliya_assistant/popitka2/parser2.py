#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
parser3.py ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–±–æ—Ä —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ —Ç–µ–º–µ –∞–ª–∏–∏ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

üí° –ß—Ç–æ –¥–µ–ª–∞–µ—Ç:
1. –ü–∞—Ä—Å–∏—Ç —Å–∞–π—Ç—ã KolZchut –∏ Gov.il, –≤–∫–ª—é—á–∞—è —Å–∫—Ä—ã—Ç—ã–µ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
2. –ù–∞—Ö–æ–¥–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–∞–∂–µ –µ—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Ç–æ–ª—å–∫–æ –≤ —Ç–µ–∫—Å—Ç–µ (–∞ –Ω–µ –≤ URL)
3. –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç, PDF, —Ñ–æ—Ä–º—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å—ë –≤ –µ–¥–∏–Ω—ã–π knowledge_base_aliyah_full.txt
4. –û—Ç—Å–µ–∏–≤–∞–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–º—ã (–∞—Ä–º–∏—è, –Ω–∞–ª–æ–≥–∏, –º–µ–¥–∏—Ü–∏–Ω–∞ –∏ —Ç.–ø.)
5. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —è–∑—ã–∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã)
6. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Ñ–æ—Ä–º—ã –æ—Ç–¥–µ–ª—å–Ω–æ –≤ –ø–∞–ø–∫—É docs/
7. –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—á—ë—Ç—á–∏–∫–∏

üì¶ –í—ã—Ö–æ–¥:
- knowledge_base_aliyah_full.txt  ‚Äî –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–∞—è –±–∞–∑–∞
- docs/                           ‚Äî —Å–∫–∞—á–∞–Ω–Ω—ã–µ PDF, DOC –∏ —Ç.–¥.
- parser3.log                     ‚Äî –ª–æ–≥-—Ñ–∞–π–ª
"""

import os
import re
import time
import logging
import requests
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from pdfminer.high_level import extract_text as pdf_extract_text


# ----------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ -----------
OUTPUT_FILE = Path("knowledge_base_aliyah_full.txt")
DOCS_DIR = Path("docs")
DOCS_DIR.mkdir(exist_ok=True)

LOG_FILE = Path("parser3.log")
logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger("parser3")

# –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
START_URLS = [
    "https://www.kolzchut.org.il/ru/–†–µ–ø–∞—Ç—Ä–∏–∞–Ω—Ç—ã",
    "https://www.kolzchut.org.il/ru/New_Olim",
    "https://www.kolzchut.org.il/ru/category/–û–ª–∏–º_–•–∞–¥–∞—à–∏–º",
    "https://www.kolzchut.org.il/ru/–¢–æ—à–∞–≤_—Ö–æ–∑–µ—Ä",
    "https://www.gov.il/ru/subjects/immigration_and_absorption",
    "https://www.gov.il/ru/subjects/returning_residents",
    "https://www.gov.il/ru/subjects/learning_hebrew",
    "https://www.gov.il/ru/departments/ministry_of_aliyah_and_integration",
    "https://govextra.gov.il/moia/your-place-in-israel-lang/home-ru/",
]

KEYWORDS = [
    "–∞–ª–∏—è", "–∞–ª–∏–º", "—Ä–µ–ø–∞—Ç—Ä–∏–∞–Ω—Ç", "—Ä–µ–ø–∞—Ç—Ä–∏–∞–Ω—Ç—ã", "–≤–æ–∑–≤—Ä–∞—â–∞—é—â",
    "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∞–ª–∏–∏", "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏", "–∞–±—Å–æ—Ä–±—Ü",
    "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è", "—É–ª—å–ø–∞–Ω", "–µ–≤—Ä–µ–π", "—Ç–æ—à–∞–≤ —Ö–æ–∑–µ—Ä", "olim", "aliyah",
    "absorption", "integration", "returning"
]

EXCLUDE = ["army", "tax", "covid", "pension", "violence", "lawyer", "children", "business"]

PDF_EXT = {".pdf"}
FORM_EXT = {".doc", ".docx", ".xls", ".xlsx", ".rtf", ".odt", ".zip"}
MAX_PAGES = 150
MAX_DEPTH = 4
DELAY = 0.5


# ----------- –£—Ç–∏–ª–∏—Ç—ã -----------
def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def is_russian_text(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ —Ö–æ—Ç—è –±—ã 25% –∫–∏—Ä–∏–ª–ª–∏—Ü—ã"""
    letters = [c for c in text if c.isalpha()]
    if not letters:
        return False
    cyr = sum(1 for c in letters if "–∞" <= c.lower() <= "—è" or c.lower() == "—ë")
    return cyr / len(letters) > 0.25


def has_keywords(text: str) -> bool:
    t = text.lower()
    return any(k in t for k in KEYWORDS)


def file_ext(url: str) -> str:
    return Path(urlparse(url).path).suffix.lower()


def extract_pdf(session, url: str, name_hint: str = "") -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª"""
    try:
        r = session.get(url, timeout=40)
        r.raise_for_status()
        filename = DOCS_DIR / (re.sub(r'[^\w\-]+', '_', name_hint)[:60] + ".pdf")
        with open(filename, "wb") as f:
            f.write(r.content)

        text = pdf_extract_text(filename)
        text = clean_text(text)
        if is_russian_text(text) and has_keywords(text):
            return text
        return ""
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ PDF {url}: {e}")
        return ""


def extract_page(session, url: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å—Å—ã–ª–∫–∏"""
    try:
        r = session.get(url, timeout=30)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {url} ({e})")
        return None

    title_tag = soup.find("h1") or soup.find("title")
    title = clean_text(title_tag.get_text()) if title_tag else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

    main = soup.find("main") or soup.body
    text_blocks = []
    if main:
        for el in main.find_all(["p", "li", "h2", "h3", "div"]):
            txt = clean_text(el.get_text())
            if txt and len(txt) > 25:
                text_blocks.append(txt)

    content = "\n".join(text_blocks)
    if not is_russian_text(content) or not has_keywords(content):
        return None

    forms, pdfs, links = [], [], []
    for a in soup.find_all("a", href=True):
        full_url = urljoin(url, a["href"])
        ext = file_ext(full_url)
        if any(x in full_url for x in ["#", "javascript:", "mailto:"]):
            continue
        if ext in FORM_EXT:
            forms.append(full_url)
        elif ext in PDF_EXT:
            pdfs.append(full_url)
        elif full_url.startswith("https://") and not any(x in full_url for x in EXCLUDE):
            links.append(full_url)

    return title, content, forms, pdfs, links


# ----------- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ -----------
def crawl():
    session = requests.Session()
    session.headers.update({"User-Agent": "Mozilla/5.0"})

    visited = {}
    queue = [(url, 0) for url in START_URLS]
    count, pdf_count, form_count = 0, 0, 0

    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        while queue and count < MAX_PAGES:
            url, depth = queue.pop(0)
            if url in visited or depth > MAX_DEPTH:
                continue
            visited[url] = True

            logger.info(f"[{count+1}] {url}")
            print(f"‚Üí [{count+1}] {url}")
            result = extract_page(session, url)
            if not result:
                continue
            title, content, forms, pdfs, links = result

            source = "kolzchut" if "kolzchut" in url else "gov.il"
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

            f.write("=" * 80 + "\n")
            f.write(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}\n")
            f.write(f"–°—Å—ã–ª–∫–∞: {url}\n")
            f.write(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n")
            f.write(f"–î–∞—Ç–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {timestamp}\n")
            f.write("-" * 80 + "\n")
            f.write(content + "\n\n")

            if forms:
                form_count += len(forms)
                f.write("–§–æ—Ä–º—ã –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è:\n")
                for form in forms:
                    f.write(f"üìÑ {form}\n")
                f.write("\n")

            for pdf_url in pdfs:
                pdf_text = extract_pdf(session, pdf_url, title)
                if pdf_text:
                    pdf_count += 1
                    f.write("–ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç (PDF):\n")
                    f.write(f"üìë {pdf_url}\n")
                    f.write(pdf_text + "\n\n")

            count += 1
            for u in links:
                if u not in visited:
                    queue.append((u, depth + 1))

            time.sleep(DELAY)

    logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω: {count} —Å—Ç—Ä–∞–Ω–∏—Ü, {pdf_count} PDF, {form_count} —Ñ–æ—Ä–º")
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! {count} —Å—Ç—Ä–∞–Ω–∏—Ü, {pdf_count} PDF, {form_count} —Ñ–æ—Ä–º.")
    print(f"üìÇ –†–µ–∑—É–ª—å—Ç–∞—Ç: {OUTPUT_FILE}")


if __name__ == "__main__":
    crawl()

